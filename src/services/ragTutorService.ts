import { supabase } from '@/lib/supabase'
import { callGeminiGenerate, GEMINI_GENERATE_MODEL } from '@/services/geminiClient'
import { ResponseCache } from '@/services/cacheService'

export async function askAITutor(
  question: string,
  courseId: string
): Promise<{ success: boolean; answer?: string; sources?: string[]; error?: any }> {
  const GEMINI_API_KEY = import.meta.env.VITE_GEMINI_API_KEY

  if (!GEMINI_API_KEY) {
    return {
      success: false,
      error: 'Gemini API key not configured. Check .env file.'
    }
  }

  try {
    // üöÄ QUICK WIN: Check cache first (40-60% faster, cheaper)
    const cachedAnswer = await ResponseCache.get(question, courseId, 'tutor')
    if (cachedAnswer) {
      console.log('‚úÖ Returning cached answer (instant response!)')
      return {
        success: true,
        answer: cachedAnswer,
        sources: ['Cached response (previously generated)']
      }
    }

    console.log('ü§ñ Generating question embedding...')

    // Step 1: Generate embedding for the question
    const questionEmbedding = await generateQuestionEmbedding(question)

    console.log('üîç Searching for relevant content...')

    // Step 2: Find relevant course content using vector similarity
    const { data: relevantContent, error: searchError } = await supabase.rpc('match_course_content', {
      query_embedding: questionEmbedding,
      course_id_param: courseId,
      match_threshold: 0.5,
      match_count: 3
    })

    if (searchError) {
      console.warn('‚ö†Ô∏è Vector search error (using fallback):', searchError)
    }

    // Step 3: Build context from retrieved content
    let context = ''
    let sources: string[] = []

    if (relevantContent && relevantContent.length > 0) {
      context = relevantContent.map((item: any) => item.content).join('\n\n---\n\n')
      sources = relevantContent.map((item: any, i: number) => `Source ${i + 1}: ${item.content.substring(0, 100)}...`)
      console.log(`‚úÖ Found ${relevantContent.length} relevant sections (similarity-based)`)
    } else {
      // Fallback: get recent modules for this course
      const { data: modules } = await supabase
        .from('course_modules')
        .select('content')
        .eq('course_id', courseId)
        .limit(2)

      if (modules && modules.length > 0) {
        context = modules.map((m) => m.content).join('\n\n---\n\n')
        sources = modules.map((m, i) => `Module ${i + 1}`)
        console.log('‚úÖ Using course modules as context')
      } else {
        context = 'General federated learning knowledge'
        console.log('‚ö†Ô∏è No content found, using general FL knowledge')
      }
    }

    console.log('ü§ñ Generating contextual answer with Gemini...')

    // Step 4: Generate answer using Gemini with RAG context
    const answer = await generateContextualAnswer(question, context)

    // üöÄ QUICK WIN: Cache the response for future use
    await ResponseCache.set(question, courseId, answer, 'tutor')

    // Step 5: Save conversation to database
    await supabase.from('chat_messages').insert([
      {
        course_id: courseId,
        message: question,
        role: 'user',
        student_id: null
      },
      {
        course_id: courseId,
        message: answer,
        role: 'assistant',
        student_id: null
      }
    ])

    console.log('‚úÖ Response generated, cached, and conversation saved')

    return { success: true, answer, sources }
  } catch (error: any) {
    console.error('‚ùå AI Tutor error:', error)
    
    // Try one more time with just course modules (no vector search)
    try {
      const { data: modules } = await supabase
        .from('course_modules')
        .select('title, content')
        .eq('course_id', courseId)
        .limit(3)
      
      if (modules && modules.length > 0) {
        const fallbackContext = modules.map(m => `**${m.title}**\n\n${m.content}`).join('\n\n---\n\n')
        const answer = await generateContextualAnswer(question, fallbackContext)
        return { 
          success: true, 
          answer, 
          sources: modules.map(m => m.title) 
        }
      }
    } catch (fallbackError) {
      console.error('‚ùå Fallback also failed:', fallbackError)
    }
    
    // Final fallback to smart responses
    const fallbackAnswer = getFallbackResponse(question)
    return { success: true, answer: fallbackAnswer, sources: ['General knowledge (no course content found)'] }
  }
}

async function generateQuestionEmbedding(question: string): Promise<number[]> {
  const GEMINI_API_KEY = import.meta.env.VITE_GEMINI_API_KEY

  const response = await fetch(
    `https://generativelanguage.googleapis.com/v1beta/models/embedding-001:embedContent?key=${GEMINI_API_KEY}`,
    {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: 'models/embedding-001',
        content: { parts: [{ text: question }] }
      })
    }
  )

  if (!response.ok) {
    throw new Error(`Embedding API error: ${response.status}`)
  }

  const data = await response.json()
  return data.embedding.values
}

async function generateContextualAnswer(question: string, context: string): Promise<string> {
  const GEMINI_API_KEY = import.meta.env.VITE_GEMINI_API_KEY

  const prompt = `You are a friendly, expert AI tutor helping a student learn about their course material. You're knowledgeable, encouraging, and explain things clearly.

RELEVANT COURSE MATERIAL (from the student's course):
${context.substring(0, 3500)}

STUDENT'S QUESTION:
"${question}"

YOUR ROLE:
- Answer the question conversationally and helpfully
- Draw primarily from the course material above, but you can supplement with your expertise
- Use examples, analogies, and real-world applications
- Break down complex concepts into digestible parts
- Be encouraging and supportive in your tone
- Use emojis occasionally to be friendly (but not excessively)
- If something isn't in the course material, say so and offer general knowledge
- Aim for 2-4 clear, well-structured paragraphs

Provide your answer as if you're having a conversation with a curious student:`

  const requestBody = {
    contents: [
      {
        parts: [{ text: prompt }]
      }
    ],
    generationConfig: {
      temperature: 0.7,
      maxOutputTokens: 1024
    }
  }

  const data = await callGeminiGenerate(requestBody, GEMINI_API_KEY, {
    model: GEMINI_GENERATE_MODEL
  })

  const aiText = data.candidates?.[0]?.content?.parts?.[0]?.text
  return aiText
}

// Fallback smart responses if everything fails
export function getFallbackResponse(question: string): string {
  const lowerQ = question.toLowerCase()

  if (lowerQ.includes('federated learning') || lowerQ.includes('what is fl')) {
    return "Federated Learning is a machine learning approach where multiple devices collaboratively train a model without sharing their raw data. Each device trains locally on its own data, then only sends model updates (gradients) to a central server. The server aggregates these updates to improve the global model. This way, your personal data never leaves your device, ensuring complete privacy! üîí\n\nThink of it like a study group where everyone learns from their own textbooks at home, then shares only their notes and insights - never the actual books."
  }

  if (lowerQ.includes('privacy') || lowerQ.includes('secure')) {
    return "Privacy in Federated Learning is maintained through several techniques:\n\n1. **Local training** - your data never leaves your device\n2. **Differential privacy** - adds mathematical noise to protect individual contributions\n3. **Secure aggregation** - the server only sees combined updates, not individual ones\n4. **Encryption** - all communications are encrypted\n\nThis makes FL perfect for sensitive applications like healthcare, finance, and mobile keyboards! üõ°Ô∏è"
  }

  if (lowerQ.includes('how') || lowerQ.includes('work') || lowerQ.includes('process')) {
    return "Here's how Federated Learning works step-by-step:\n\n1. üì§ The server sends a global model to all participating devices\n2. üß† Each device trains the model locally on its own data\n3. üìä Devices send only the model updates (gradients) back to the server\n4. üîÑ The server aggregates all updates using an algorithm like FedAvg\n5. üì• The improved global model is sent back to devices\n6. üîÅ This process repeats until the model converges\n\nThe key insight: **data stays local, only knowledge (model updates) is shared!**"
  }

  if (lowerQ.includes('advantage') || lowerQ.includes('benefit')) {
    return "Federated Learning offers several key advantages:\n\n1. **Privacy** üîí - Data never leaves user devices\n2. **Compliance** üìã - Meets GDPR and data protection regulations\n3. **Scalability** üìà - Leverages distributed computing power\n4. **Personalization** üéØ - Models can adapt to local data patterns\n5. **Reduced bandwidth** üì° - Only model updates are transmitted\n6. **Access to diverse data** üåç - Learn from data that couldn't be centralized\n\nThese benefits make FL ideal for mobile applications, healthcare, and any privacy-sensitive domain!"
  }

  if (lowerQ.includes('challenge') || lowerQ.includes('problem') || lowerQ.includes('difficult')) {
    return "Federated Learning faces several challenges:\n\n1. **Non-IID data** üìä - Each device has different data distributions\n2. **Communication costs** üí∏ - Frequent model updates can be expensive\n3. **Device heterogeneity** üì± - Different hardware capabilities\n4. **Privacy-utility tradeoff** ‚öñÔ∏è - More privacy protection can reduce accuracy\n5. **Stragglers** üêå - Slow devices can delay training rounds\n\nResearchers are actively working on solutions like adaptive aggregation, compression, and asynchronous training!"
  }

  if (lowerQ.includes('example') || lowerQ.includes('use case') || lowerQ.includes('application')) {
    return "Real-world Federated Learning applications:\n\nüì± **Gboard (Google Keyboard)** - Learns from your typing without seeing your messages\n\nüè• **Healthcare** - Hospitals collaborate on disease prediction models without sharing patient data\n\nüí∞ **Finance** - Banks detect fraud collectively while keeping transactions private\n\nüéµ **Spotify/Apple Music** - Personalize recommendations without centralized listening history\n\nüöó **Autonomous vehicles** - Learn from road conditions across fleets without uploading dashcam footage\n\nFL is revolutionizing how we build AI while respecting privacy!"
  }

  // Default intelligent response
  return `That's a great question about **"${question.substring(0, 50)}${question.length > 50 ? '...' : ''}"**!\n\nIn the context of Federated Learning, this relates to how we enable collaborative machine learning while preserving data privacy. The core principle is that your data stays on your device - only model improvements are shared with the server.\n\nCould you be more specific about what aspect you'd like to explore? For example:\n\n‚Ä¢ How the training process works?\n‚Ä¢ Privacy guarantees?\n‚Ä¢ Real-world applications?\n‚Ä¢ Technical challenges?\n\nI'm here to help you understand FL concepts better! üöÄ`
}
