# ========================================
# E-Learning Platform Environment Variables
# ========================================

# === Core Services ===
VITE_SUPABASE_URL=your-supabase-url
VITE_SUPABASE_PUBLISHABLE_KEY=your-supabase-anon-key

# === AI Services (Primary) ===
VITE_GEMINI_API_KEY=your-gemini-api-key

# === AI Services (Fallback - Phase 2A) ===
# OpenRouter (optional fallback, pay-as-you-go)
VITE_OPENROUTER_API_KEY=your-openrouter-key
VITE_OPENROUTER_MODEL=anthropic/claude-3-haiku  # or google/gemini-flash-1.5

# Ollama (optional local fallback, free)
VITE_OLLAMA_BASE_URL=http://localhost:11434
VITE_OLLAMA_MODEL=llama2  # or mistral, phi, etc.

# === Monitoring (Optional) ===
VITE_SENTRY_DSN=your-sentry-dsn

# === Azure Functions (Legacy - not required for Phase 1-2) ===
VITE_API_URL=http://localhost:7071/api

# ========================================
# How to Use LLM Fallback (Phase 2A)
# ========================================
#
# The system automatically falls back through providers in this order:
# 1. Gemini (primary) - Always tries first if VITE_GEMINI_API_KEY is set
# 2. OpenRouter (fallback) - Tries if Gemini fails and VITE_OPENROUTER_API_KEY is set
# 3. Ollama (local) - Last resort if VITE_OLLAMA_BASE_URL is set
#
# To enable OpenRouter:
# 1. Get API key from https://openrouter.ai/
# 2. Set VITE_OPENROUTER_API_KEY=your-key
# 3. Choose model (haiku is fast/cheap, flash is balanced)
#
# To enable Ollama (local, free):
# 1. Install: https://ollama.ai/
# 2. Run: ollama pull llama2
# 3. Start: ollama serve
# 4. Set VITE_OLLAMA_BASE_URL=http://localhost:11434
#
# Benefits:
# - Eliminates 429 rate limit errors
# - Automatic retry with exponential backoff
# - Zero downtime (switches providers automatically)
# - Cost optimization (tries free Gemini first)
# - Local fallback available (Ollama)
#
# ========================================
